{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Initial Setup and Configuration\n",
        "\n",
        "This notebook sets up the Databricks environment and loads initial data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Required Libraries\n",
        "\n",
        "Install all required Python packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install yfinance>=0.2.0\n",
        "%pip install alpha-vantage>=2.3.0\n",
        "%pip install fredapi>=0.5.0\n",
        "%pip install openai>=1.0.0\n",
        "%pip install anthropic>=0.18.0\n",
        "%pip install pydantic>=2.0.0\n",
        "%pip install python-dotenv>=1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure API Keys\n",
        "\n",
        "Set up API keys from Databricks Secrets or environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Option 1: Use Databricks Secrets (Recommended)\n",
        "try:\n",
        "    openai_key = dbutils.secrets.get(scope=\"stocks_ai_secrets\", key=\"openai_api_key\")\n",
        "    os.environ['OPENAI_API_KEY'] = openai_key\n",
        "    print(\"✓ OpenAI API key loaded from secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ OpenAI key not found in secrets: {e}\")\n",
        "\n",
        "try:\n",
        "    alpha_vantage_key = dbutils.secrets.get(scope=\"stocks_ai_secrets\", key=\"alpha_vantage_api_key\")\n",
        "    os.environ['ALPHA_VANTAGE_API_KEY'] = alpha_vantage_key\n",
        "    print(\"✓ Alpha Vantage API key loaded from secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Alpha Vantage key not found in secrets: {e}\")\n",
        "\n",
        "try:\n",
        "    fred_key = dbutils.secrets.get(scope=\"stocks_ai_secrets\", key=\"fred_api_key\")\n",
        "    os.environ['FRED_API_KEY'] = fred_key\n",
        "    print(\"✓ FRED API key loaded from secrets\")\n",
        "except Exception as e:\n",
        "    print(f\"⚠ FRED key not found in secrets: {e}\")\n",
        "\n",
        "# Option 2: Use environment variables (if set on cluster)\n",
        "# Keys should already be in os.environ if set on cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Set Up Python Path\n",
        "\n",
        "Automatically detect repository path or set manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Try to auto-detect repository path\n",
        "repo_path = None\n",
        "\n",
        "# Option 1: Check if running in Databricks Repos\n",
        "try:\n",
        "    # Get current notebook path\n",
        "    notebook_path = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get()\n",
        "    if '/Repos/' in notebook_path:\n",
        "        # Extract repo path\n",
        "        parts = notebook_path.split('/Repos/')\n",
        "        if len(parts) > 1:\n",
        "            repo_base = '/Workspace/Repos/' + parts[1].split('/')[0] + '/stocks-ai-system'\n",
        "            if os.path.exists(repo_base):\n",
        "                repo_path = repo_base\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Option 2: Check common workspace locations\n",
        "if not repo_path:\n",
        "    possible_paths = [\n",
        "        '/Workspace/Repos/stocks-ai-system',\n",
        "        '/Workspace/Users/' + os.getenv('USER', 'user') + '/stocks-ai-system',\n",
        "    ]\n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            repo_path = path\n",
        "            break\n",
        "\n",
        "# Option 3: Manual override (uncomment and set if auto-detection fails)\n",
        "# repo_path = '/Workspace/Repos/your-username/stocks-ai-system'\n",
        "\n",
        "if repo_path and repo_path not in sys.path:\n",
        "    sys.path.insert(0, repo_path)\n",
        "    print(f\"✓ Added {repo_path} to Python path\")\n",
        "elif repo_path:\n",
        "    print(f\"✓ Path already configured: {repo_path}\")\n",
        "else:\n",
        "    print(\"⚠ Could not auto-detect repository path. Please set repo_path manually.\")\n",
        "    print(\"Current working directory:\", os.getcwd())\n",
        "    print(\"Python path:\", sys.path[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Test Basic Data Loading\n",
        "\n",
        "Test Yahoo Finance before full implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "from datetime import date\n",
        "\n",
        "# Test Yahoo Finance\n",
        "print(\"Testing Yahoo Finance...\")\n",
        "ticker = yf.Ticker(\"AAPL\")\n",
        "info = ticker.info\n",
        "print(f\"✓ Company: {info.get('longName', 'N/A')}\")\n",
        "print(f\"✓ Sector: {info.get('sector', 'N/A')}\")\n",
        "print(f\"✓ Industry: {info.get('industry', 'N/A')}\")\n",
        "\n",
        "# Test price data\n",
        "hist = ticker.history(period=\"5d\")\n",
        "if not hist.empty:\n",
        "    print(f\"✓ Latest price: ${hist['Close'].iloc[-1]:.2f}\")\n",
        "    print(f\"✓ Data loaded successfully!\")\n",
        "else:\n",
        "    print(\"⚠ No price data available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load Fortune 100 Companies\n",
        "\n",
        "Load initial company master data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from datetime import datetime\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Top 100 US companies by market capitalization (Fortune 100 equivalent)\n",
        "# These are the largest and most liquid stocks, ideal for stock prediction\n",
        "fortune100_symbols = [\n",
        "    # Top 20 - Mega Cap\n",
        "    \"AAPL\", \"MSFT\", \"GOOGL\", \"AMZN\", \"NVDA\",\n",
        "    \"META\", \"TSLA\", \"BRK.B\", \"V\", \"JNJ\",\n",
        "    \"WMT\", \"PG\", \"MA\", \"UNH\", \"HD\",\n",
        "    \"DIS\", \"BAC\", \"ADBE\", \"NFLX\", \"CRM\",\n",
        "    # 21-40 - Large Cap\n",
        "    \"AVGO\", \"XOM\", \"JPM\", \"LLY\", \"COST\",\n",
        "    \"CVX\", \"ABBV\", \"PEP\", \"MRK\", \"TMO\",\n",
        "    \"ACN\", \"CSCO\", \"MCD\", \"WFC\", \"ABT\",\n",
        "    \"LIN\", \"DHR\", \"VZ\", \"NKE\", \"PM\",\n",
        "    # 41-60 - Large Cap continued\n",
        "    \"TXN\", \"CMCSA\", \"BMY\", \"UPS\", \"QCOM\",\n",
        "    \"AMGN\", \"RTX\", \"HON\", \"AMAT\", \"INTU\",\n",
        "    \"LOW\", \"DE\", \"BKNG\", \"SPGI\", \"ADP\",\n",
        "    \"GE\", \"C\", \"ELV\", \"SBUX\", \"TMUS\",\n",
        "    # 61-80 - Mid-Large Cap\n",
        "    \"ISRG\", \"GILD\", \"AXP\", \"ADI\", \"ANET\",\n",
        "    \"CDNS\", \"SNPS\", \"KLAC\", \"FTNT\", \"NXPI\",\n",
        "    \"MCHP\", \"APH\", \"CTSH\", \"PAYX\", \"FAST\",\n",
        "    \"CTAS\", \"WDAY\", \"ZS\", \"CRWD\", \"CDW\",\n",
        "    # 81-100 - Additional Large/Mid Cap\n",
        "    \"FDS\", \"KEYS\", \"BR\", \"POOL\", \"NDAQ\",\n",
        "    \"CPRT\", \"GGG\", \"ROL\", \"TECH\", \"AOS\",\n",
        "    \"WSO\", \"RBC\", \"GWW\", \"ITW\", \"ETN\",\n",
        "    \"EMR\", \"PH\", \"ROK\", \"AME\", \"SWK\"\n",
        "]\n",
        "\n",
        "# Load companies with progress tracking\n",
        "companies = []\n",
        "failed_symbols = []\n",
        "total_symbols = len(fortune100_symbols)\n",
        "\n",
        "print(f\"Loading {total_symbols} companies from Yahoo Finance...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for idx, symbol in enumerate(fortune100_symbols, 1):\n",
        "    try:\n",
        "        ticker = yf.Ticker(symbol)\n",
        "        info = ticker.info\n",
        "        \n",
        "        companies.append({\n",
        "            \"symbol\": symbol,\n",
        "            \"company_name\": info.get(\"longName\", symbol),\n",
        "            \"sector\": info.get(\"sector\", \"Unknown\"),\n",
        "            \"industry\": info.get(\"industry\", \"Unknown\"),\n",
        "            \"market_cap\": info.get(\"marketCap\"),\n",
        "            \"fortune_rank\": None,\n",
        "            \"added_date\": datetime.now(),\n",
        "            \"updated_date\": datetime.now()\n",
        "        })\n",
        "        \n",
        "        # Show progress every 10 companies or for first/last\n",
        "        if idx % 10 == 0 or idx <= 5 or idx > total_symbols - 5:\n",
        "            print(f\"[{idx}/{total_symbols}] ✓ {symbol}: {info.get('longName', symbol)[:50]}\")\n",
        "    except Exception as e:\n",
        "        failed_symbols.append((symbol, str(e)))\n",
        "        if idx <= 10 or idx > total_symbols - 10:  # Show first/last failures\n",
        "            print(f\"[{idx}/{total_symbols}] ✗ Failed {symbol}: {str(e)[:50]}\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  ✓ Successfully loaded: {len(companies)} companies\")\n",
        "print(f\"  ✗ Failed to load: {len(failed_symbols)} companies\")\n",
        "\n",
        "if failed_symbols:\n",
        "    print(f\"\\nFailed symbols: {', '.join([s[0] for s in failed_symbols])}\")\n",
        "\n",
        "# Save to Delta table\n",
        "if companies:\n",
        "    try:\n",
        "        # Define explicit schema to handle None values properly\n",
        "        from pyspark.sql.types import StructType, StructField, StringType, LongType, IntegerType, TimestampType\n",
        "        \n",
        "        schema = StructType([\n",
        "            StructField(\"symbol\", StringType(), nullable=False),\n",
        "            StructField(\"company_name\", StringType(), nullable=True),\n",
        "            StructField(\"sector\", StringType(), nullable=True),\n",
        "            StructField(\"industry\", StringType(), nullable=True),\n",
        "            StructField(\"market_cap\", LongType(), nullable=True),\n",
        "            StructField(\"fortune_rank\", IntegerType(), nullable=True),\n",
        "            StructField(\"added_date\", TimestampType(), nullable=True),\n",
        "            StructField(\"updated_date\", TimestampType(), nullable=True)\n",
        "        ])\n",
        "        \n",
        "        # Ensure data types are correct for Spark\n",
        "        from pyspark.sql import Row\n",
        "        from datetime import datetime as dt\n",
        "        \n",
        "        # Convert companies list to properly typed rows\n",
        "        typed_companies = []\n",
        "        for company in companies:\n",
        "            # Convert market_cap to long (BIGINT) or None\n",
        "            market_cap = company.get('market_cap')\n",
        "            if market_cap is not None:\n",
        "                try:\n",
        "                    # Handle both int and float from Yahoo Finance\n",
        "                    market_cap = int(float(market_cap))\n",
        "                except (ValueError, TypeError, OverflowError):\n",
        "                    market_cap = None\n",
        "            \n",
        "            # Convert datetime to Timestamp - Spark expects datetime objects or None\n",
        "            added_date = company.get('added_date')\n",
        "            updated_date = company.get('updated_date')\n",
        "            \n",
        "            typed_companies.append({\n",
        "                \"symbol\": str(company.get('symbol', '')),\n",
        "                \"company_name\": str(company.get('company_name', '')) if company.get('company_name') else None,\n",
        "                \"sector\": str(company.get('sector', '')) if company.get('sector') else None,\n",
        "                \"industry\": str(company.get('industry', '')) if company.get('industry') else None,\n",
        "                \"market_cap\": market_cap,\n",
        "                \"fortune_rank\": None,  # Explicitly None for IntegerType\n",
        "                \"added_date\": added_date if isinstance(added_date, dt) else None,\n",
        "                \"updated_date\": updated_date if isinstance(updated_date, dt) else None\n",
        "            })\n",
        "        \n",
        "        companies = typed_companies\n",
        "        \n",
        "        # Create DataFrame with explicit schema\n",
        "        df = spark.createDataFrame(companies, schema=schema)\n",
        "        \n",
        "        # Verify the DataFrame schema matches\n",
        "        print(f\"\\nDataFrame schema:\")\n",
        "        df.printSchema()\n",
        "        \n",
        "        # Check if table exists, if not create it first\n",
        "        try:\n",
        "            spark.sql(\"DESCRIBE TABLE stocks_ai.fortune100.companies\").show()\n",
        "            print(\"✓ Table exists\")\n",
        "        except Exception:\n",
        "            print(\"⚠ Table does not exist - will be created on write\")\n",
        "            # Create table first using SQL\n",
        "            spark.sql(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS stocks_ai.fortune100.companies (\n",
        "                  symbol STRING NOT NULL,\n",
        "                  company_name STRING,\n",
        "                  sector STRING,\n",
        "                  industry STRING,\n",
        "                  market_cap BIGINT,\n",
        "                  fortune_rank INT,\n",
        "                  added_date TIMESTAMP,\n",
        "                  updated_date TIMESTAMP\n",
        "                ) USING DELTA\n",
        "            \"\"\")\n",
        "            print(\"✓ Table created\")\n",
        "        \n",
        "        # Write to table\n",
        "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"stocks_ai.fortune100.companies\")\n",
        "        print(f\"\\n✓ Saved {len(companies)} companies to Delta table: stocks_ai.fortune100.companies\")\n",
        "        \n",
        "        # Show sample of loaded companies\n",
        "        print(\"\\nSample of loaded companies:\")\n",
        "        df.select(\"symbol\", \"company_name\", \"sector\").show(20, truncate=False)\n",
        "        \n",
        "        # Show sector distribution\n",
        "        print(\"\\nSector distribution:\")\n",
        "        df.groupBy(\"sector\").count().orderBy(\"count\", ascending=False).show(truncate=False)\n",
        "    except Exception as e:\n",
        "        print(f\"\\n✗ Error saving to Delta table: {e}\")\n",
        "        print(\"  Make sure you ran setup/init.sql in SQL Editor first!\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(\"\\n✗ No companies loaded - cannot save to Delta table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Verify SQL Setup\n",
        "\n",
        "First, verify that Unity Catalog schemas were created by running `setup/init.sql` in SQL Editor.\n",
        "Then check that the companies table exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql\n",
        "-- Verify catalog exists\n",
        "SHOW CATALOGS LIKE 'stocks_ai';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "%sql\n",
        "-- Verify schemas exist\n",
        "SHOW SCHEMAS IN stocks_ai;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%sql\n",
        "-- Verify companies table exists and check count\n",
        "SELECT COUNT(*) as company_count FROM stocks_ai.fortune100.companies;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Set Up MLflow Experiment\n",
        "\n",
        "Create MLflow experiment for tracking predictions and models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import mlflow\n",
        "\n",
        "# Track MLflow setup success\n",
        "mlflow_experiment_success = False\n",
        "mlflow_logging_success = False\n",
        "\n",
        "# Create or set MLflow experiment\n",
        "experiment_path = \"/Repos/krish.kilaru@lumenalta.com/stocks-ai-system/notebooks/00_initial_setup\"\n",
        "try:\n",
        "    mlflow.set_experiment(experiment_path)\n",
        "    print(f\"✓ MLflow experiment set: {experiment_path}\")\n",
        "    mlflow_experiment_success = True\n",
        "except Exception as e:\n",
        "    print(f\"⚠ Could not set MLflow experiment: {e}\")\n",
        "    print(\"You may need to create it manually or check permissions\")\n",
        "\n",
        "# Test logging\n",
        "try:\n",
        "    with mlflow.start_run(run_name=\"phase1_setup_test\"):\n",
        "        mlflow.log_param(\"phase\", \"phase1_setup\")\n",
        "        mlflow.log_param(\"test\", \"setup_verification\")\n",
        "        # Use globals() for Databricks notebooks (variables are in global scope)\n",
        "        companies_count = len(companies) if 'companies' in globals() and companies else 0\n",
        "        mlflow.log_metric(\"companies_loaded\", companies_count)\n",
        "        print(\"✓ MLflow logging test successful\")\n",
        "        mlflow_logging_success = True\n",
        "except Exception as e:\n",
        "    print(f\"⚠ MLflow logging test failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SETUP VERIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Fix Bug 1: Use globals() instead of locals() for Databricks notebooks\n",
        "# In Databricks, variables are shared in global scope across cells\n",
        "repo_path_exists = 'repo_path' in globals() and repo_path is not None\n",
        "companies_exists = 'companies' in globals() and companies is not None\n",
        "\n",
        "checks = {\n",
        "    \"Python path configured\": repo_path_exists and repo_path in sys.path,\n",
        "    \"OpenAI key available\": os.getenv('OPENAI_API_KEY') is not None,\n",
        "    \"Yahoo Finance works\": companies_exists and len(companies) > 0,\n",
        "    \"Delta table accessible\": False,\n",
        "    # Fix Bug 2: Check actual MLflow setup result instead of hardcoding True\n",
        "    \"MLflow configured\": mlflow_experiment_success and mlflow_logging_success\n",
        "}\n",
        "\n",
        "# Verify Delta table\n",
        "try:\n",
        "    count = spark.sql(\"SELECT COUNT(*) as cnt FROM stocks_ai.fortune100.companies\").collect()[0]['cnt']\n",
        "    checks[\"Delta table accessible\"] = True\n",
        "    checks[\"Companies loaded\"] = count > 0\n",
        "    print(f\"✓ Companies in table: {count}\")\n",
        "except Exception as e:\n",
        "    print(f\"✗ Delta table error: {e}\")\n",
        "    print(\"  Make sure you ran setup/init.sql in SQL Editor first!\")\n",
        "\n",
        "for check, status in checks.items():\n",
        "    status_icon = \"✓\" if status else \"✗\"\n",
        "    print(f\"{status_icon} {check}\")\n",
        "\n",
        "# Final status\n",
        "all_critical = all([\n",
        "    checks.get(\"Python path configured\", False),\n",
        "    checks.get(\"Yahoo Finance works\", False),\n",
        "    checks.get(\"Delta table accessible\", False)\n",
        "])\n",
        "\n",
        "if all_critical:\n",
        "    print(\"\\n✅ Phase 1 Setup Complete!\")\n",
        "    print(\"You can now proceed to Phase 2: Core Infrastructure\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Some critical checks failed. Please review:\")\n",
        "    print(\"  1. Make sure you ran setup/init.sql in SQL Editor\")\n",
        "    print(\"  2. Verify Python path is set correctly\")\n",
        "    print(\"  3. Check that companies were loaded successfully\")"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+json": {
      "notebookMetadata": {
        "pythonIndentUnit": 4
      }
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
